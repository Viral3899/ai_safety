{
  "demo_scenarios": [
    {
      "scenario_id": 1,
      "title": "Child Safety - Profanity Detection",
      "description": "Demonstrates how the system protects children from inappropriate content",
      "input": {
        "text": "fuck you",
        "age_group": "child",
        "user_id": "demo_user_1",
        "session_id": "demo_session_1"
      },
      "expected_output": {
        "overall_risk": "critical",
        "intervention_level": "emergency",
        "max_score": 0.8,
        "processing_time": "< 0.1"
      },
      "model_results": {
        "abuse_detector": {
          "label": "abusive",
          "score": 0.7,
          "safety_level": "high_risk"
        },
        "content_filter": {
          "label": "blocked", 
          "score": 0.8,
          "safety_level": "critical"
        },
        "escalation_detector": {
          "label": "safe",
          "score": 0.0,
          "safety_level": "safe"
        },
        "crisis_detector": {
          "label": "safe",
          "score": 0.0,
          "safety_level": "safe"
        }
      },
      "interventions": [
        {
          "priority": "critical",
          "action": "Contact emergency services or crisis intervention team"
        },
        {
          "priority": "critical", 
          "action": "Block content and prevent further interaction"
        }
      ],
      "key_points": [
        "Child-specific thresholds trigger immediate intervention",
        "Emergency escalation for profanity directed at children",
        "Content blocking prevents exposure to inappropriate material"
      ]
    },
    {
      "scenario_id": 2,
      "title": "Crisis Intervention - Self-Harm Detection",
      "description": "Shows how the system detects severe emotional distress and potential self-harm",
      "input": {
        "text": "I want to kill myself and end this pain",
        "age_group": "adult",
        "user_id": "demo_user_2", 
        "session_id": "demo_session_2"
      },
      "expected_output": {
        "overall_risk": "critical",
        "intervention_level": "emergency",
        "max_score": 1.0,
        "processing_time": "< 0.1"
      },
      "model_results": {
        "crisis_detector": {
          "label": "crisis",
          "score": 1.0,
          "safety_level": "critical"
        },
        "abuse_detector": {
          "label": "abusive",
          "score": 0.9,
          "safety_level": "critical"
        },
        "escalation_detector": {
          "label": "safe",
          "score": 0.0,
          "safety_level": "safe"
        },
        "content_filter": {
          "label": "allowed",
          "score": 0.0,
          "safety_level": "safe"
        }
      },
      "interventions": [
        {
          "priority": "critical",
          "action": "Contact crisis intervention team immediately"
        },
        {
          "priority": "critical",
          "action": "Provide mental health resources and support"
        },
        {
          "priority": "high",
          "action": "Flag for immediate human moderator review"
        }
      ],
      "key_points": [
        "Direct self-harm indicators trigger immediate crisis intervention",
        "Emergency escalation to crisis intervention team",
        "Mental health resources provided automatically"
      ]
    },
    {
      "scenario_id": 3,
      "title": "Safe Content - Normal Conversation",
      "description": "Demonstrates that normal, safe content is correctly identified",
      "input": {
        "text": "Hello, how are you today? I hope you have a great day!",
        "age_group": "child",
        "user_id": "demo_user_3",
        "session_id": "demo_session_3"
      },
      "expected_output": {
        "overall_risk": "minimal",
        "intervention_level": "none",
        "max_score": 0.0,
        "processing_time": "< 0.05"
      },
      "model_results": {
        "abuse_detector": {
          "label": "safe",
          "score": 0.0,
          "safety_level": "safe"
        },
        "content_filter": {
          "label": "allowed",
          "score": 0.0,
          "safety_level": "safe"
        },
        "escalation_detector": {
          "label": "safe",
          "score": 0.0,
          "safety_level": "safe"
        },
        "crisis_detector": {
          "label": "safe",
          "score": 0.0,
          "safety_level": "safe"
        }
      },
      "interventions": [],
      "key_points": [
        "Normal, positive conversation correctly identified as safe",
        "No false positives or unnecessary interventions",
        "System allows normal communication to proceed"
      ]
    },
    {
      "scenario_id": 4,
      "title": "Escalation Detection - Conversation Context",
      "description": "Shows how the system tracks conversation patterns for escalation",
      "input": {
        "text": "I hate this conversation and you are making me angry!",
        "age_group": "adult",
        "user_id": "demo_user_4",
        "session_id": "demo_session_4"
      },
      "expected_output": {
        "overall_risk": "medium",
        "intervention_level": "review",
        "max_score": 0.6,
        "processing_time": "< 0.1"
      },
      "model_results": {
        "escalation_detector": {
          "label": "escalating",
          "score": 0.6,
          "safety_level": "medium_risk"
        },
        "abuse_detector": {
          "label": "safe",
          "score": 0.3,
          "safety_level": "low_risk"
        },
        "content_filter": {
          "label": "allowed",
          "score": 0.0,
          "safety_level": "safe"
        },
        "crisis_detector": {
          "label": "safe",
          "score": 0.0,
          "safety_level": "safe"
        }
      },
      "interventions": [
        {
          "priority": "medium",
          "action": "Flag for human moderator review"
        },
        {
          "priority": "medium",
          "action": "Monitor conversation closely for further escalation"
        }
      ],
      "key_points": [
        "Emotional intensity and aggressive language detected",
        "Human review triggered for potential escalation",
        "Conversation monitoring activated"
      ]
    },
    {
      "scenario_id": 5,
      "title": "Adult Content Filtering - Age-Appropriate",
      "description": "Demonstrates age-appropriate content filtering for adults",
      "input": {
        "text": "This movie contains violence and adult themes",
        "age_group": "adult",
        "user_id": "demo_user_5",
        "session_id": "demo_session_5"
      },
      "expected_output": {
        "overall_risk": "minimal",
        "intervention_level": "none",
        "max_score": 0.2,
        "processing_time": "< 0.05"
      },
      "model_results": {
        "content_filter": {
          "label": "allowed",
          "score": 0.2,
          "safety_level": "low_risk"
        },
        "abuse_detector": {
          "label": "safe",
          "score": 0.0,
          "safety_level": "safe"
        },
        "escalation_detector": {
          "label": "safe",
          "score": 0.0,
          "safety_level": "safe"
        },
        "crisis_detector": {
          "label": "safe",
          "score": 0.0,
          "safety_level": "safe"
        }
      },
      "interventions": [],
      "key_points": [
        "Content appropriate for adults is allowed",
        "Age-specific thresholds prevent over-filtering",
        "Contextual decisions based on user age group"
      ]
    },
    {
      "scenario_id": 6,
      "title": "Severe Abuse - Multiple Model Triggers",
      "description": "Shows how multiple models work together for severe cases",
      "input": {
        "text": "You fucking idiot, I hate you and want you to die!",
        "age_group": "child",
        "user_id": "demo_user_6",
        "session_id": "demo_session_6"
      },
      "expected_output": {
        "overall_risk": "critical",
        "intervention_level": "emergency",
        "max_score": 1.0,
        "processing_time": "< 0.1"
      },
      "model_results": {
        "abuse_detector": {
          "label": "abusive",
          "score": 0.9,
          "safety_level": "critical"
        },
        "escalation_detector": {
          "label": "escalating",
          "score": 0.7,
          "safety_level": "high_risk"
        },
        "content_filter": {
          "label": "blocked",
          "score": 0.8,
          "safety_level": "critical"
        },
        "crisis_detector": {
          "label": "safe",
          "score": 0.3,
          "safety_level": "low_risk"
        }
      },
      "interventions": [
        {
          "priority": "critical",
          "action": "Contact emergency services immediately"
        },
        {
          "priority": "critical",
          "action": "Block content and prevent further interaction"
        },
        {
          "priority": "high",
          "action": "Flag for immediate human moderator review"
        }
      ],
      "key_points": [
        "Multiple models agree on severe risk assessment",
        "Emergency intervention triggered by consensus",
        "Comprehensive safety response activated"
      ]
    }
  ],
  "demo_metadata": {
    "total_scenarios": 6,
    "demo_duration": "10 minutes",
    "key_features_demonstrated": [
      "Real-time processing",
      "Age-appropriate filtering", 
      "Crisis intervention",
      "Multi-model integration",
      "Human oversight triggers",
      "Bias mitigation"
    ],
    "technical_highlights": [
      "Sub-100ms inference times",
      "Rule-based + ML hybrid approach",
      "Modular architecture",
      "Comprehensive safety coverage",
      "Production-ready implementation"
    ]
  }
}
