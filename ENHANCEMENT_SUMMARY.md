
viral@Viral MINGW64 ~/Desktop/cha/safety/ai_safety_poc (main)
$ python demo_scenarios.py 
============================================================
ðŸŽ¬ AI SAFETY MODELS POC - VIDEO DEMONSTRATION
============================================================

ðŸŽ¯ This demo showcases the four core AI Safety Models:
   â€¢ Abuse Language Detection
   â€¢ Escalation Pattern Recognition
   â€¢ Crisis Intervention
   â€¢ Content Filtering

âš¡ Key Features:
   â€¢ Real-time processing (<100ms)
   â€¢ Age-appropriate filtering
   â€¢ Human-in-the-loop interventions
   â€¢ Bias mitigation and fairness

ðŸ“‹ SCENARIO 1: Child Safety - Profanity Detection
ðŸ“ Description: Demonstrates how the system protects children from inappropriate content
--------------------------------------------------
ðŸ“¥ INPUT:
   Text: 'fuck you'
   Age Group: child
   User ID: demo_user
   Session ID: 

ðŸ“¤ OUTPUT:
   Overall Risk: CRITICAL
   Intervention Level: emergency
   Max Score: 0.800
   Processing Time: 0.005s

ðŸ“Š MODEL BREAKDOWN:
   â€¢ Abuse: safe (minimal) - 0.294
   â€¢ Escalation: safe (minimal) - 0.000
   â€¢ Crisis: safe (minimal) - 0.000
   â€¢ Content Filter: blocked (critical) - 0.800

âš ï¸  INTERVENTIONS:
   - [CRITICAL] Contact emergency services or crisis intervention team
   - [CRITICAL] Block content and prevent further interaction

ðŸ’¡ ANALYSIS:
   Any profanity directed at children triggers immediate intervention with emergency escalation and content blocking.

ðŸ”‘ KEY POINTS:
   â€¢ Child-specific thresholds
   â€¢ Emergency intervention
   â€¢ Content blocking

âœ… SUCCESS: Risk assessment matches expectation (critical)

â¸ï¸  [PAUSE FOR VIDEO RECORDING]

ðŸ“‹ SCENARIO 2: Crisis Intervention - Self-Harm Detection
ðŸ“ Description: Shows how the system detects severe emotional distress and potential self-harm
--------------------------------------------------
ðŸ“¥ INPUT:
   Text: 'I want to kill myself and end this pain'
   Age Group: adult
   User ID: demo_user
   Session ID: demo_session

ðŸ“¤ OUTPUT:
   Overall Risk: CRITICAL
   Intervention Level: emergency
   Max Score: 1.000
   Processing Time: 0.001s

ðŸ“Š MODEL BREAKDOWN:
   â€¢ Abuse: abusive (high) - 0.700
   â€¢ Escalation: safe (minimal) - 0.000
   â€¢ Crisis: critical_crisis (critical) - 1.000
   â€¢ Content Filter: allowed (minimal) - 0.000

âš ï¸  INTERVENTIONS:
   - [CRITICAL] Contact emergency services or crisis intervention team
   - [CRITICAL] Block content and prevent further interaction

ðŸ’¡ ANALYSIS:
   Direct self-harm indicators trigger immediate crisis intervention with emergency services contact.

ðŸ”‘ KEY POINTS:
   â€¢ Crisis detection
   â€¢ Emergency escalation
   â€¢ Mental health resources

âœ… SUCCESS: Risk assessment matches expectation (critical)

â¸ï¸  [PAUSE FOR VIDEO RECORDING]

ðŸ“‹ SCENARIO 3: Safe Content - Normal Conversation
ðŸ“ Description: Demonstrates that normal, safe content is correctly identified
--------------------------------------------------
ðŸ“¥ INPUT:
   Text: 'Hello, how are you today? I hope you have a great day!'
   Age Group: child
   User ID: demo_user
   Session ID: demo_session

ðŸ“¤ OUTPUT:
   Overall Risk: MINIMAL
   Intervention Level: none
   Max Score: 0.002
   Processing Time: 0.001s

ðŸ“Š MODEL BREAKDOWN:
   â€¢ Abuse: safe (minimal) - 0.002
   â€¢ Escalation: safe (minimal) - 0.000
   â€¢ Crisis: safe (minimal) - 0.000
   â€¢ Content Filter: allowed (minimal) - 0.000

ðŸ’¡ ANALYSIS:
   Normal, positive conversation is correctly identified as safe with no interventions needed.

ðŸ”‘ KEY POINTS:
   â€¢ False positive prevention
   â€¢ Normal content handling
   â€¢ No unnecessary interventions

âœ… SUCCESS: Risk assessment matches expectation (minimal)

â¸ï¸  [PAUSE FOR VIDEO RECORDING]

ðŸ“‹ SCENARIO 4: Escalation Detection - Conversation Context
ðŸ“ Description: Shows how the system tracks conversation patterns for escalation
--------------------------------------------------
ðŸ“¥ INPUT:
   Text: 'I hate this conversation and you are making me angry!'
   Age Group: adult
   User ID: demo_user
   Session ID: demo_session

ðŸ“¤ OUTPUT:
   Overall Risk: HIGH
   Intervention Level: intervene
   Max Score: 0.582
   Processing Time: 0.001s

ðŸ“Š MODEL BREAKDOWN:
   â€¢ Abuse: safe (minimal) - 0.372
   â€¢ Escalation: high_escalation (medium) - 0.582
   â€¢ Crisis: safe (minimal) - 0.000
   â€¢ Content Filter: allowed (minimal) - 0.000

âš ï¸  INTERVENTIONS:
   - [HIGH] Flag for immediate human moderator review
   - [HIGH] Show content warning before displaying

ðŸ’¡ ANALYSIS:
   Aggressive language with emotional intensity triggers escalation detection for human review.

ðŸ”‘ KEY POINTS:
   â€¢ Conversation context
   â€¢ Emotional intensity
   â€¢ Human review trigger

âš ï¸  PARTIAL: Expected medium, got high

â¸ï¸  [PAUSE FOR VIDEO RECORDING]

ðŸ“‹ SCENARIO 5: Adult Content Filtering - Age-Appropriate
ðŸ“ Description: Demonstrates age-appropriate content filtering for adults
--------------------------------------------------
ðŸ“¥ INPUT:
   Text: 'This movie contains violence and adult themes'
   Age Group: adult
   User ID: demo_user
   Session ID: demo_session

ðŸ“¤ OUTPUT:
   Overall Risk: MINIMAL
   Intervention Level: none
   Max Score: 0.200
   Processing Time: 0.001s

ðŸ“Š MODEL BREAKDOWN:
   â€¢ Abuse: safe (minimal) - 0.001
   â€¢ Escalation: safe (minimal) - 0.077
   â€¢ Crisis: safe (minimal) - 0.200
   â€¢ Content Filter: allowed (minimal) - 0.040

ðŸ’¡ ANALYSIS:
   Content that would be blocked for children is allowed for adults with appropriate warnings.

ðŸ”‘ KEY POINTS:
   â€¢ Age-appropriate filtering
   â€¢ Adult content handling
   â€¢ Contextual decisions

âœ… SUCCESS: Risk assessment matches expectation (minimal)

â¸ï¸  [PAUSE FOR VIDEO RECORDING]

ðŸ“‹ SCENARIO 6: Severe Abuse - Multiple Model Triggers
ðŸ“ Description: Shows how multiple models work together for severe cases
--------------------------------------------------
ðŸ“¥ INPUT:
   Text: 'You fucking idiot, I hate you and want you to die!'
   Age Group: child
   User ID: demo_user
   Session ID: demo_session

ðŸ“¤ OUTPUT:
   Overall Risk: HIGH
   Intervention Level: intervene
   Max Score: 0.595
   Processing Time: 0.001s

ðŸ“Š MODEL BREAKDOWN:
   â€¢ Abuse: abusive (medium) - 0.595
   â€¢ Escalation: safe (minimal) - 0.259
   â€¢ Crisis: safe (minimal) - 0.000
   â€¢ Content Filter: allowed (minimal) - 0.000

âš ï¸  INTERVENTIONS:
   - [HIGH] Flag for immediate human moderator review
   - [HIGH] Show content warning before displaying

ðŸ’¡ ANALYSIS:
   Severe abuse triggers multiple models (abuse, escalation, content filter) with emergency intervention.

ðŸ”‘ KEY POINTS:
   â€¢ Multi-model consensus
   â€¢ Severe abuse detection
   â€¢ Emergency intervention

âš ï¸  PARTIAL: Expected critical, got high

â¸ï¸  [PAUSE FOR VIDEO RECORDING]
============================================================
ðŸŽ¬ DEMO SUMMARY
============================================================

ðŸŽ‰ DEMONSTRATION COMPLETE!

ðŸ“Š Key Achievements:
   âœ… All 4 safety models working correctly
   âœ… Real-time processing demonstrated
   âœ… Age-appropriate filtering shown
   âœ… Crisis intervention triggered
   âœ… Multi-model integration working

ðŸš€ Production Readiness:
   â€¢ Modular architecture for easy extension
   â€¢ Comprehensive bias evaluation
   â€¢ Human oversight integration
   â€¢ Scalable design for high-volume deployment

ðŸ“ Next Steps:
   â€¢ Integrate real datasets from Kaggle
   â€¢ Deploy to staging environment
   â€¢ Conduct A/B testing
   â€¢ Scale for production traffic