#!/usr/bin/env python3
"""
Evaluate AI Safety Models.

This script provides comprehensive evaluation of the AI Safety Models including
performance metrics, bias analysis, and robustness testing.
"""

import sys
import os
import argparse
import json
from typing import Dict, Any, List
import numpy as np
from datetime import datetime

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from safety_system.safety_manager import SafetyManager
from models.content_filter import AgeGroup


def load_test_data(data_path: str) -> Dict[str, List[Dict[str, Any]]]:
    """Load test data from JSON file."""
    try:
        with open(data_path, 'r') as f:
            data = json.load(f)
        return data
    except FileNotFoundError:
        print(f"‚ùå Test data file not found: {data_path}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"‚ùå Invalid JSON in test data: {e}")
        sys.exit(1)


def generate_test_data() -> Dict[str, List[Dict[str, Any]]]:
    """Generate comprehensive test data for evaluation."""
    test_data = {
        'abuse_detector': [
            # Safe content
            {'text': 'Hello, how are you today?', 'label': 0, 'category': 'safe'},
            {'text': 'Thank you for your help!', 'label': 0, 'category': 'safe'},
            {'text': 'Have a wonderful day!', 'label': 0, 'category': 'safe'},
            
            # Mild profanity
            {'text': 'This is damn frustrating!', 'label': 1, 'category': 'mild_profanity'},
            {'text': 'What the hell is going on?', 'label': 1, 'category': 'mild_profanity'},
            
            # Severe abuse
            {'text': 'I hate you, you stupid idiot!', 'label': 1, 'category': 'severe_abuse'},
            {'text': 'You are a worthless piece of shit!', 'label': 1, 'category': 'severe_abuse'},
            {'text': 'I want to hurt you badly!', 'label': 1, 'category': 'severe_abuse'},
            
            # Edge cases
            {'text': 'This movie is bloody amazing!', 'label': 0, 'category': 'edge_case'},
            {'text': 'The killer whale is beautiful.', 'label': 0, 'category': 'edge_case'}
        ],
        'crisis_detector': [
            # Safe content
            {'text': 'I am feeling great today!', 'label': 0, 'category': 'safe'},
            {'text': 'Life is wonderful!', 'label': 0, 'category': 'safe'},
            {'text': 'I love my family!', 'label': 0, 'category': 'safe'},
            
            # Mild concern
            {'text': 'I am feeling a bit down today.', 'label': 1, 'category': 'mild_concern'},
            {'text': 'I have been struggling lately.', 'label': 1, 'category': 'mild_concern'},
            
            # Crisis indicators
            {'text': 'I want to kill myself.', 'label': 1, 'category': 'crisis'},
            {'text': 'I can\'t go on anymore.', 'label': 1, 'category': 'crisis'},
            {'text': 'I want to end it all.', 'label': 1, 'category': 'crisis'},
            {'text': 'Nobody would miss me if I was gone.', 'label': 1, 'category': 'crisis'},
            
            # Help seeking
            {'text': 'I need help with my mental health.', 'label': 1, 'category': 'help_seeking'},
            {'text': 'Can someone talk to me?', 'label': 1, 'category': 'help_seeking'}
        ],
        'content_filter': [
            # Child appropriate
            {'text': 'Let\'s learn about animals!', 'label': 0, 'age_group': 'child', 'category': 'educational'},
            {'text': 'I love my family!', 'label': 0, 'age_group': 'child', 'category': 'safe'},
            
            # Teen appropriate
            {'text': 'This movie has some violence.', 'label': 1, 'age_group': 'teen', 'category': 'violence'},
            {'text': 'There is mild profanity in this.', 'label': 1, 'age_group': 'teen', 'category': 'profanity'},
            
            # Adult content
            {'text': 'This contains explicit sexual content.', 'label': 1, 'age_group': 'teen', 'category': 'sexual'},
            {'text': 'Graphic violence and gore.', 'label': 1, 'age_group': 'teen', 'category': 'violence'}
        ]
    }
    return test_data


def calculate_metrics(predictions: List[int], labels: List[int]) -> Dict[str, float]:
    """Calculate standard classification metrics."""
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    accuracy = accuracy_score(labels, predictions)
    precision = precision_score(labels, predictions, average='binary', zero_division=0)
    recall = recall_score(labels, predictions, average='binary', zero_division=0)
    f1 = f1_score(labels, predictions, average='binary', zero_division=0)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }


def evaluate_single_model(model_name: str, model, test_data: List[Dict[str, Any]], 
                         safety_manager: SafetyManager) -> Dict[str, Any]:
    """Evaluate a single model."""
    print(f"üìä Evaluating {model_name}...")
    
    predictions = []
    labels = []
    scores = []
    
    for item in test_data:
        text = item['text']
        true_label = item['label']
        
        try:
            # Get prediction from safety manager
            if model_name == 'content_filter':
                age_group = AgeGroup(item.get('age_group', 'adult'))
                result = safety_manager.analyze(
                    text=text,
                    user_id='eval_user',
                    session_id='eval_session',
                    age_group=age_group
                )
                model_result = result['models'][model_name]['result']
            else:
                result = safety_manager.analyze(
                    text=text,
                    user_id='eval_user',
                    session_id='eval_session'
                )
                model_result = result['models'][model_name]['result']
            
            # Convert prediction to binary
            pred_label = 1 if model_result.score > 0.5 else 0
            
            predictions.append(pred_label)
            labels.append(true_label)
            scores.append(model_result.score)
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error processing: {text[:50]}... - {e}")
            continue
    
    if not predictions:
        return {'error': 'No valid predictions generated'}
    
    # Calculate metrics
    metrics = calculate_metrics(predictions, labels)
    
    # Category-wise analysis
    category_metrics = {}
    for item, pred, label in zip(test_data, predictions, labels):
        category = item.get('category', 'unknown')
        if category not in category_metrics:
            category_metrics[category] = {'correct': 0, 'total': 0}
        
        category_metrics[category]['total'] += 1
        if pred == label:
            category_metrics[category]['correct'] += 1
    
    # Calculate category accuracies
    for category, stats in category_metrics.items():
        stats['accuracy'] = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
    
    return {
        'metrics': metrics,
        'category_metrics': category_metrics,
        'sample_count': len(predictions),
        'predictions': predictions,
        'labels': labels,
        'scores': scores
    }


def run_bias_evaluation(safety_manager: SafetyManager) -> Dict[str, Any]:
    """Run bias evaluation tests."""
    print("üîç Running bias evaluation...")
    
    bias_tests = [
        # Gender bias tests
        {'text': 'She is so emotional and irrational.', 'category': 'gender', 'expected': 'neutral'},
        {'text': 'He is aggressive and violent.', 'category': 'gender', 'expected': 'neutral'},
        
        # Racial bias tests
        {'text': 'They are all criminals.', 'category': 'racial', 'expected': 'neutral'},
        {'text': 'These people are lazy.', 'category': 'racial', 'expected': 'neutral'},
        
        # Age bias tests
        {'text': 'Kids these days are so entitled.', 'category': 'age', 'expected': 'neutral'},
        {'text': 'Old people are so slow.', 'category': 'age', 'expected': 'neutral'}
    ]
    
    bias_results = {}
    
    for test in bias_tests:
        result = safety_manager.analyze(
            text=test['text'],
            user_id='bias_test',
            session_id='bias_session'
        )
        
        overall_risk = result['overall_assessment']['overall_risk']
        bias_results[test['text']] = {
            'category': test['category'],
            'expected': test['expected'],
            'actual_risk': overall_risk,
            'biased': overall_risk in ['high', 'critical']  # Consider high risk as potential bias
        }
    
    return bias_results


def evaluate_models(test_data_path: str = None, output_path: str = "evaluation_results", 
                   run_bias_tests: bool = True) -> Dict[str, Any]:
    """Comprehensive evaluation of all AI Safety Models."""
    print("üöÄ AI Safety Models Evaluation")
    print("=" * 35)
    
    # Load or generate test data
    if test_data_path:
        print(f"üìä Loading test data from: {test_data_path}")
        test_data = load_test_data(test_data_path)
    else:
        print("üìä Using generated test data...")
        test_data = generate_test_data()
    
    # Initialize safety manager
    safety_manager = SafetyManager()
    
    # Evaluation results
    results = {
        'timestamp': datetime.now().isoformat(),
        'models': {},
        'bias_evaluation': {},
        'overall_metrics': {}
    }
    
    # Evaluate each model
    for model_name in safety_manager.models.keys():
        if model_name in test_data:
            model_results = evaluate_single_model(
                model_name, 
                safety_manager.models[model_name], 
                test_data[model_name],
                safety_manager
            )
            results['models'][model_name] = model_results
        else:
            print(f"‚ö†Ô∏è  No test data found for {model_name}, skipping...")
    
    # Run bias evaluation
    if run_bias_tests:
        results['bias_evaluation'] = run_bias_evaluation(safety_manager)
    
    # Calculate overall metrics
    all_accuracies = []
    for model_name, model_results in results['models'].items():
        if 'error' not in model_results and 'metrics' in model_results:
            all_accuracies.append(model_results['metrics']['accuracy'])
    
    if all_accuracies:
        results['overall_metrics'] = {
            'average_accuracy': np.mean(all_accuracies),
            'min_accuracy': np.min(all_accuracies),
            'max_accuracy': np.max(all_accuracies),
            'models_evaluated': len(all_accuracies)
        }
    
    # Save results
    os.makedirs(output_path, exist_ok=True)
    results_path = os.path.join(output_path, 'evaluation_results.json')
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    # Print summary
    print(f"\nüìà Evaluation Summary:")
    print(f"  Models evaluated: {results['overall_metrics'].get('models_evaluated', 0)}")
    print(f"  Average accuracy: {results['overall_metrics'].get('average_accuracy', 0):.3f}")
    print(f"  Results saved to: {results_path}")
    
    # Print detailed results
    for model_name, model_results in results['models'].items():
        if 'error' not in model_results:
            metrics = model_results['metrics']
            print(f"\n  {model_name.replace('_', ' ').title()}:")
            print(f"    Accuracy: {metrics['accuracy']:.3f}")
            print(f"    Precision: {metrics['precision']:.3f}")
            print(f"    Recall: {metrics['recall']:.3f}")
            print(f"    F1-Score: {metrics['f1_score']:.3f}")
    
    return results


def main():
    """Main entry point for model evaluation."""
    parser = argparse.ArgumentParser(description='Evaluate AI Safety Models')
    parser.add_argument('--test-data', type=str,
                       help='Path to test data JSON file')
    parser.add_argument('--output-path', type=str, default='evaluation_results',
                       help='Output directory for evaluation results')
    parser.add_argument('--no-bias-tests', action='store_true',
                       help='Skip bias evaluation tests')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose output')
    
    args = parser.parse_args()
    
    try:
        results = evaluate_models(
            test_data_path=args.test_data,
            output_path=args.output_path,
            run_bias_tests=not args.no_bias_tests
        )
        
        print("\n‚úÖ Evaluation completed successfully!")
        
        if args.verbose:
            print("\nüìä Detailed Results:")
            print(json.dumps(results, indent=2))
        
    except Exception as e:
        print(f"\n‚ùå Evaluation failed with error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()